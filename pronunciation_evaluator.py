import torch
import numpy as np
import Levenshtein
import re
from model_utils import transcribe, convert_to_ipa
from phoneme_extractor import phoneme_extractor, extract_phonemes
import librosa

class PronunciationEvaluator:
    def __init__(self):
        """Kh·ªüi t·∫°o ƒë√°nh gi√° ph√°t √¢m"""
        # T·∫£i b·ªô t·ª´ ƒëi·ªÉn ph√°t √¢m CMU (Carnegie Mellon University) n·∫øu c√≥
        # Ho·∫∑c s·ª≠ d·ª•ng c√°c t√†i nguy√™n ph√°t √¢m kh√°c
        self.phoneme_weights = {
            # Tr·ªçng s·ªë cho c√°c l·ªói ph√°t √¢m th∆∞·ªùng g·∫∑p (v·ªõi ng∆∞·ªùi n√≥i ti·∫øng Vi·ªát)
            'Œ∏': 2.0,  # th kh√≥ ph√°t √¢m
            '√∞': 2.0,  # th voiced kh√≥ ph√°t √¢m
            '…π': 1.5,  # r kh√≥ ph√°t √¢m
            ' É': 1.5,  # sh kh√≥ ph√°t √¢m
            ' í': 1.5,  # zh kh√≥ ph√°t √¢m
            '√¶': 1.5,  # √¢m √¶ kh√≥ ph√°t √¢m
            '…ô': 1.2,  # schwa kh√≥ ph√°t √¢m
        }
        
    def get_standard_pronunciation(self, text):
        """
        L·∫•y ph√°t √¢m chu·∫©n cho m·ªôt ƒëo·∫°n vƒÉn b·∫£n ti·∫øng Anh
        Trong th·ª±c t·∫ø, b·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng t·ª´ ƒëi·ªÉn ph√°t √¢m nh∆∞ CMUdict
        """
        # ƒê√¢y l√† m·ªôt gi·∫£i ph√°p t·∫°m th·ªùi, ·ª©ng d·ª•ng th·ª±c t·∫ø c·∫ßn t·ª´ ƒëi·ªÉn ph√°t √¢m
        return convert_to_ipa(text)
        
    def dynamic_time_warping(self, seq1, seq2):
        """
        T√≠nh to√°n DTW (Dynamic Time Warping) gi·ªØa hai chu·ªói k√Ω t·ª±
        """
        n, m = len(seq1), len(seq2)
        dtw_matrix = np.zeros((n+1, m+1))
        
        for i in range(n+1):
            for j in range(m+1):
                if i == 0 and j == 0:
                    dtw_matrix[i, j] = 0
                elif i == 0:
                    dtw_matrix[i, j] = dtw_matrix[i, j-1] + 1
                elif j == 0:
                    dtw_matrix[i, j] = dtw_matrix[i-1, j] + 1
                else:
                    cost = 0 if seq1[i-1] == seq2[j-1] else 1
                    dtw_matrix[i, j] = cost + min(dtw_matrix[i-1, j],      # insertion
                                                dtw_matrix[i, j-1],      # deletion
                                                dtw_matrix[i-1, j-1])    # substitution
                    
        return dtw_matrix[n, m]
        
    def weighted_levenshtein_distance(self, ref_ipa, user_ipa):
        """
        T√≠nh kho·∫£ng c√°ch Levenshtein c√≥ tr·ªçng s·ªë gi·ªØa phi√™n √¢m chu·∫©n v√† phi√™n √¢m ng∆∞·ªùi d√πng
        """
        n, m = len(ref_ipa), len(user_ipa)
        dp = [[0 for _ in range(m + 1)] for _ in range(n + 1)]
        
        for i in range(n + 1):
            dp[i][0] = i
        for j in range(m + 1):
            dp[0][j] = j
        
        for i in range(1, n + 1):
            for j in range(1, m + 1):
                if ref_ipa[i-1] == user_ipa[j-1]:
                    dp[i][j] = dp[i-1][j-1]
                else:
                    weight = self.phoneme_weights.get(ref_ipa[i-1], 1.0)
                    dp[i][j] = min(
                        dp[i-1][j] + weight,  # deletion
                        dp[i][j-1] + 1.0,     # insertion
                        dp[i-1][j-1] + weight # substitution
                    )
        
        return dp[n][m]
        
    def analyze_rhythm_and_stress(self, audio_path, reference_text):
        """
        Ph√¢n t√≠ch nh·ªãp ƒëi·ªáu v√† tr·ªçng √¢m
        """
        try:
            # T·∫£i √¢m thanh
            y, sr = librosa.load(audio_path, sr=None)
            
            # Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng
            tempo, _ = librosa.beat.beat_track(y=y, sr=sr)
            onset_env = librosa.onset.onset_strength(y=y, sr=sr)
            rhythm_regularity = np.std(onset_env)
            
            # Ph√¢n t√≠ch tr·ªçng √¢m th√¥ng qua bi√™n ƒë·ªô
            rms = librosa.feature.rms(y=y)[0]
            stress_contrast = np.max(rms) / np.mean(rms)
            
            return {
                "tempo": tempo,
                "rhythm_regularity": float(rhythm_regularity),
                "stress_contrast": float(stress_contrast)
            }
        except Exception as e:
            print(f"Error analyzing rhythm: {str(e)}")
            return {
                "tempo": 0,
                "rhythm_regularity": 0,
                "stress_contrast": 0
            }
    
    def evaluate(self, audio_path, reference_text):
        """
        ƒê√°nh gi√° ph√°t √¢m
        
        Args:
            audio_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file √¢m thanh ng∆∞·ªùi d√πng
            reference_text: VƒÉn b·∫£n tham chi·∫øu (ti·∫øng Anh)
            
        Returns:
            Dict ch·ª©a k·∫øt qu·∫£ ƒë√°nh gi√°
        """
        # NEW METHOD: L·∫•y phi√™n √¢m tr·ª±c ti·∫øp t·ª´ √¢m thanh (kh√¥ng qua vƒÉn b·∫£n)
        user_phonemes = extract_phonemes(audio_path)
        user_text = transcribe(audio_path, output_ipa=False)  # V·∫´n l·∫•y vƒÉn b·∫£n ƒë·ªÉ hi·ªÉn th·ªã
        user_ipa_direct = user_phonemes["ipa"]  # Phi√™n √¢m IPA tr·ª±c ti·∫øp t·ª´ √¢m thanh
        
        # L∆∞u c·∫£ phi√™n √¢m gi√°n ti·∫øp (qua vƒÉn b·∫£n) ƒë·ªÉ so s√°nh v·ªõi ph∆∞∆°ng ph√°p c≈©
        user_result = transcribe(audio_path, output_ipa=True)
        user_ipa_indirect = user_result["ipa"]  # Phi√™n √¢m IPA gi√°n ti·∫øp qua vƒÉn b·∫£n
        
        # L·∫•y ph√°t √¢m chu·∫©n (ground truth)
        reference_ipa = self.get_standard_pronunciation(reference_text)
        
        # T√≠nh ƒëi·ªÉm ƒë·ªô ch√≠nh x√°c vƒÉn b·∫£n (ASR accuracy)
        text_distance = Levenshtein.distance(reference_text.lower(), user_text.lower())
        text_max_len = max(len(reference_text), len(user_text))
        text_accuracy = ((text_max_len - text_distance) / text_max_len) * 100 if text_max_len > 0 else 100
        
        # T√≠nh ƒëi·ªÉm ƒë·ªô ch√≠nh x√°c phi√™n √¢m tr·ª±c ti·∫øp (Direct Phoneme accuracy)
        direct_ipa_distance = self.weighted_levenshtein_distance(reference_ipa, user_ipa_direct)
        direct_ipa_max_len = max(len(reference_ipa), len(user_ipa_direct))
        direct_pronunciation_accuracy = ((direct_ipa_max_len - direct_ipa_distance) / direct_ipa_max_len) * 100 if direct_ipa_max_len > 0 else 100
        
        # T√≠nh ƒëi·ªÉm ƒë·ªô ch√≠nh x√°c phi√™n √¢m gi√°n ti·∫øp (qua vƒÉn b·∫£n) cho tham chi·∫øu
        indirect_ipa_distance = self.weighted_levenshtein_distance(reference_ipa, user_ipa_indirect)
        indirect_ipa_max_len = max(len(reference_ipa), len(user_ipa_indirect))
        indirect_pronunciation_accuracy = ((indirect_ipa_max_len - indirect_ipa_distance) / indirect_ipa_max_len) * 100 if indirect_ipa_max_len > 0 else 100
        
        # So s√°nh ƒë·ªô ch√≠nh x√°c gi·ªØa hai ph∆∞∆°ng ph√°p phi√™n √¢m
        print(f"üîç ƒê·ªô ch√≠nh x√°c phi√™n √¢m tr·ª±c ti·∫øp: {direct_pronunciation_accuracy:.2f}%")
        print(f"üîç ƒê·ªô ch√≠nh x√°c phi√™n √¢m gi√°n ti·∫øp: {indirect_pronunciation_accuracy:.2f}%")
        
        # S·ª≠ d·ª•ng ph∆∞∆°ng ph√°p c√≥ ƒë·ªô ch√≠nh x√°c cao h∆°n ƒë·ªÉ ƒë√°nh gi√°
        pronunciation_accuracy = max(direct_pronunciation_accuracy, indirect_pronunciation_accuracy)
        user_ipa = user_ipa_direct if direct_pronunciation_accuracy >= indirect_pronunciation_accuracy else user_ipa_indirect
        
        # Ph√¢n t√≠ch nh·ªãp ƒëi·ªáu v√† tr·ªçng √¢m
        rhythm_analysis = self.analyze_rhythm_and_stress(audio_path, reference_text)
        
        # T√≠nh ƒëi·ªÉm t·ªïng h·ª£p
        # 60% ph√°t √¢m, 20% vƒÉn b·∫£n, 20% nh·ªãp ƒëi·ªáu
        rhythm_score = min(100, rhythm_analysis["stress_contrast"] * 20)
        total_score = 0.6 * pronunciation_accuracy + 0.2 * text_accuracy + 0.2 * rhythm_score
        
        # X√°c ƒë·ªãnh m·ª©c ƒë·ªô ph√°t √¢m
        level = "Beginner"
        if total_score >= 90:
            level = "Native-like"
        elif total_score >= 80:
            level = "Advanced"
        elif total_score >= 70:
            level = "Intermediate"
        elif total_score >= 60:
            level = "Pre-intermediate"
            
        # T√¨m l·ªói c·ª• th·ªÉ b·∫±ng c√°ch so s√°nh t·ª´ng t·ª´
        errors = self.identify_specific_errors(reference_text, user_text, reference_ipa, user_ipa)
        
        # Ph√¢n t√≠ch phi√™n √¢m tr·ª±c ti·∫øp
        phoneme_comparison = phoneme_extractor.compare_phonemes(reference_ipa, user_ipa_direct)
        
        return {
            "score": round(total_score, 2),
            "level": level,
            "details": {
                "pronunciation_accuracy": round(pronunciation_accuracy, 2),
                "direct_pronunciation_accuracy": round(direct_pronunciation_accuracy, 2),
                "indirect_pronunciation_accuracy": round(indirect_pronunciation_accuracy, 2),
                "text_accuracy": round(text_accuracy, 2),
                "rhythm_score": round(rhythm_score, 2),
                "tempo": round(rhythm_analysis["tempo"], 2),
                "phoneme_similarity": phoneme_comparison["similarity"]
            },
            "reference": {
                "text": reference_text,
                "ipa": reference_ipa
            },
            "user": {
                "text": user_text,
                "ipa": user_ipa,
                "direct_ipa": user_ipa_direct,
                "indirect_ipa": user_ipa_indirect,
                "raw_phonemes": user_phonemes["raw_phonemes"]
            },
            "errors": errors,
            "method": "direct" if direct_pronunciation_accuracy >= indirect_pronunciation_accuracy else "indirect"
        }
    
    def identify_specific_errors(self, ref_text, user_text, ref_ipa, user_ipa):
        """
        X√°c ƒë·ªãnh c√°c l·ªói ph√°t √¢m c·ª• th·ªÉ
        """
        errors = []
        
        # Chuy·ªÉn th√†nh c√°c t·ª´
        ref_words = ref_text.lower().split()
        user_words = user_text.lower().split()
        
        # T√¨m t·ª´ thi·∫øu
        missing = set(ref_words) - set(user_words)
        if missing:
            errors.append({
                "type": "missing_words",
                "description": f"Thi·∫øu c√°c t·ª´: {', '.join(missing)}"
            })
        
        # T√¨m t·ª´ th·ª´a
        extra = set(user_words) - set(ref_words)
        if extra:
            errors.append({
                "type": "extra_words",
                "description": f"Th√™m c√°c t·ª´ kh√¥ng c√≥ trong vƒÉn b·∫£n: {', '.join(extra)}"
            })
        
        # Ph√¢n t√≠ch l·ªói th∆∞·ªùng g·∫∑p
        common_errors = self.analyze_common_errors(ref_ipa, user_ipa)
        if common_errors:
            errors.extend(common_errors)
            
        return errors
    
    def analyze_common_errors(self, ref_ipa, user_ipa):
        """
        Ph√¢n t√≠ch c√°c l·ªói ph√°t √¢m ph·ªï bi·∫øn
        """
        errors = []
        
        # Ki·ªÉm tra √¢m "th"
        if 'Œ∏' in ref_ipa and 'Œ∏' not in user_ipa:
            errors.append({
                "type": "th_sound",
                "description": "L·ªói ph√°t √¢m √¢m 'th' nh∆∞ trong 'think'"
            })
        
        # Ki·ªÉm tra √¢m "th" voiced
        if '√∞' in ref_ipa and '√∞' not in user_ipa:
            errors.append({
                "type": "th_voiced_sound",
                "description": "L·ªói ph√°t √¢m √¢m 'th' nh∆∞ trong 'the'"
            })
            
        # Ki·ªÉm tra √¢m "r"
        if '…π' in ref_ipa and '…π' not in user_ipa:
            errors.append({
                "type": "r_sound",
                "description": "L·ªói ph√°t √¢m √¢m 'r' ti·∫øng Anh"
            })
            
        # Th√™m nhi·ªÅu ph√¢n t√≠ch kh√°c...
        
        return errors

# Singleton instance
evaluator = PronunciationEvaluator()

def evaluate_pronunciation(audio_path, reference_text):
    """
    H√†m wrapper ƒë·ªÉ ƒë√°nh gi√° ph√°t √¢m
    """
    return evaluator.evaluate(audio_path, reference_text)