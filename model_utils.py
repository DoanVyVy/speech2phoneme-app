from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, Wav2Vec2FeatureExtractor
import torch
import torchaudio
import librosa
import os
import numpy as np
from pydub import AudioSegment
import tempfile
import soundfile as sf
import io
import re

# Create a directory for the tokenizer files if it doesn't exist
os.makedirs("tokenizer_data", exist_ok=True)

# Load feature extractor and tokenizer for English speech recognition
print("üîÅ ƒêang t·∫£i m√¥ h√¨nh t·ª´ Hugging Face...")
# S·ª≠ d·ª•ng m√¥ h√¨nh ti·∫øng Anh t·ªët h∆°n t·ª´ Facebook
MODEL_ID = "facebook/wav2vec2-large-960h-lv60-self"
processor = Wav2Vec2Processor.from_pretrained(MODEL_ID)
model = Wav2Vec2ForCTC.from_pretrained(MODEL_ID)
model.eval()
print("‚úÖ M√¥ h√¨nh ƒë√£ s·∫µn s√†ng!")

# B·∫£ng √°nh x·∫° phi√™n √¢m chu·∫©n ARPABET sang IPA
# ARPABET l√† chu·∫©n ƒë∆∞·ª£c s·ª≠ d·ª•ng r·ªông r√£i cho phi√™n √¢m ti·∫øng Anh M·ªπ
ARPABET_TO_IPA = {
    'AA': '…ë', 'AA0': '…ë', 'AA1': 'Àà…ë', 'AA2': 'Àå…ë',
    'AE': '√¶', 'AE0': '√¶', 'AE1': 'Àà√¶', 'AE2': 'Àå√¶',
    'AH': ' å', 'AH0': '…ô', 'AH1': 'Àà å', 'AH2': 'Àå å',
    'AO': '…î', 'AO0': '…î', 'AO1': 'Àà…î', 'AO2': 'Àå…î',
    'AW': 'a ä', 'AW0': 'a ä', 'AW1': 'Ààa ä', 'AW2': 'Àåa ä',
    'AY': 'a…™', 'AY0': 'a…™', 'AY1': 'Ààa…™', 'AY2': 'Àåa…™',
    'B': 'b',
    'CH': 't É',
    'D': 'd',
    'DH': '√∞',
    'EH': '…õ', 'EH0': '…õ', 'EH1': 'Àà…õ', 'EH2': 'Àå…õ',
    'ER': '…ù', 'ER0': '…ù', 'ER1': 'Àà…ù', 'ER2': 'Àå…ù',
    'EY': 'e…™', 'EY0': 'e…™', 'EY1': 'Ààe…™', 'EY2': 'Àåe…™',
    'F': 'f',
    'G': '…°',
    'HH': 'h',
    'IH': '…™', 'IH0': '…™', 'IH1': 'Àà…™', 'IH2': 'Àå…™',
    'IY': 'i', 'IY0': 'i', 'IY1': 'Àài', 'IY2': 'Àåi',
    'JH': 'd í',
    'K': 'k',
    'L': 'l',
    'M': 'm',
    'N': 'n',
    'NG': '≈ã',
    'OW': 'o ä', 'OW0': 'o ä', 'OW1': 'Àào ä', 'OW2': 'Àåo ä',
    'OY': '…î…™', 'OY0': '…î…™', 'OY1': 'Àà…î…™', 'OY2': 'Àå…î…™',
    'P': 'p',
    'R': '…π',
    'S': 's',
    'SH': ' É',
    'T': 't',
    'TH': 'Œ∏',
    'UH': ' ä', 'UH0': ' ä', 'UH1': 'Àà ä', 'UH2': 'Àå ä',
    'UW': 'u', 'UW0': 'u', 'UW1': 'Ààu', 'UW2': 'Àåu',
    'V': 'v',
    'W': 'w',
    'Y': 'j',
    'Z': 'z',
    'ZH': ' í',
    ' ': ' ',
}

# B·ªï sung b·∫£ng √°nh x·∫° t·ª´ k√Ω t·ª± th∆∞·ªùng sang IPA cho c√°c tr∆∞·ªùng h·ª£p kh√¥ng r√µ r√†ng
CHAR_TO_IPA = {
    'a': '√¶',
    'b': 'b',
    'c': 'k',
    'd': 'd',
    'e': '…õ',
    'f': 'f',
    'g': 'g',
    'h': 'h',
    'i': '…™',
    'j': 'd í',
    'k': 'k',
    'l': 'l',
    'm': 'm',
    'n': 'n',
    'o': '…ë',
    'p': 'p',
    'q': 'k',
    'r': '…π',
    's': 's',
    't': 't',
    'u': ' å',
    'v': 'v',
    'w': 'w',
    'x': 'ks',
    'y': 'j',
    'z': 'z',
    'th': 'Œ∏',
    'sh': ' É',
    'ch': 't É',
    'zh': ' í',
    'ng': '≈ã',
    ' ': ' ',
}

def load_audio_file(file_path, target_sr=16000):
    """
    Load audio file using multiple fallback methods
    """
    print(f"üìÇ Loading audio file: {file_path}")
    
    try:
        # Method 1: Try using pydub (handles many formats)
        try:
            audio_segment = AudioSegment.from_file(file_path)
            # Convert to wav format with required sample rate
            audio_segment = audio_segment.set_frame_rate(target_sr)
            
            # Convert to numpy array
            samples = np.array(audio_segment.get_array_of_samples(), dtype=np.float32)
            
            # Convert to mono if stereo
            if audio_segment.channels > 1:
                samples = samples.reshape((-1, audio_segment.channels)).mean(axis=1)
            
            # Normalize
            samples = samples / (2**15) if samples.dtype == np.int16 else samples / np.max(np.abs(samples))
            
            print("‚úÖ Audio loaded using pydub")
            return samples, target_sr
            
        except Exception as e:
            print(f"‚ö†Ô∏è Pydub loading failed: {str(e)}")
            
        # Method 2: Try using torchaudio
        try:
            waveform, sample_rate = torchaudio.load(file_path)
            # Convert to mono if needed
            if waveform.shape[0] > 1:
                waveform = torch.mean(waveform, dim=0, keepdim=True)
            
            # Resample if needed
            if sample_rate != target_sr:
                resampler = torchaudio.transforms.Resample(sample_rate, target_sr)
                waveform = resampler(waveform)
                
            waveform = waveform.squeeze().numpy()
            print("‚úÖ Audio loaded using torchaudio")
            return waveform, target_sr
            
        except Exception as e:
            print(f"‚ö†Ô∏è Torchaudio loading failed: {str(e)}")
            
        # Method 3: Try using soundfile
        try:
            audio, sr = sf.read(file_path)
            if sr != target_sr:
                # Resample using numpy (simple method)
                audio = librosa.resample(audio, orig_sr=sr, target_sr=target_sr)
            print("‚úÖ Audio loaded using soundfile")
            return audio, target_sr
            
        except Exception as e:
            print(f"‚ö†Ô∏è Soundfile loading failed: {str(e)}")
            
        # Method 4: Last resort, try librosa
        audio, sr = librosa.load(file_path, sr=target_sr)
        print("‚úÖ Audio loaded using librosa")
        return audio, target_sr
        
    except Exception as e:
        raise RuntimeError(f"Could not load audio file {file_path}: {str(e)}")

def extract_words_from_transcript(transcript):
    """
    Tr√≠ch xu·∫•t c√°c t·ª´ t·ª´ b·∫£n phi√™n √¢m th√¥
    """
    # Lo·∫°i b·ªè c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát v√† chuy·ªÉn th√†nh ch·ªØ th∆∞·ªùng
    cleaned = re.sub(r'[^a-zA-Z\s]', '', transcript).lower()
    # T√°ch th√†nh c√°c t·ª´
    words = cleaned.split()
    return words

def convert_to_ipa(text):
    """
    Chuy·ªÉn ƒë·ªïi vƒÉn b·∫£n th√†nh k√Ω hi·ªáu IPA
    """
    try:
        words = extract_words_from_transcript(text)
        ipa_words = []
        
        for word in words:
            # Chuy·ªÉn ƒë·ªïi t·ª´ng t·ª´ sang IPA
            ipa_word = ""
            i = 0
            while i < len(word):
                # Ki·ªÉm tra c√°c t·ªï h·ª£p 2 k√Ω t·ª± tr∆∞·ªõc
                if i < len(word) - 1 and word[i:i+2] in CHAR_TO_IPA:
                    ipa_word += CHAR_TO_IPA[word[i:i+2]]
                    i += 2
                # N·∫øu kh√¥ng, x·ª≠ l√Ω t·ª´ng k√Ω t·ª±
                elif word[i] in CHAR_TO_IPA:
                    ipa_word += CHAR_TO_IPA[word[i]]
                    i += 1
                else:
                    ipa_word += word[i]
                    i += 1
            
            ipa_words.append(ipa_word)
        
        # Gh√©p c√°c t·ª´ IPA
        return ' '.join(ipa_words)
    except Exception as e:
        print(f"‚ö†Ô∏è Error converting to IPA: {str(e)}")
        return text

def transcribe(audio_path, output_ipa=True):
    """
    Process audio file and transcribe to phonemes
    """
    # Load audio with better error handling
    audio, sr = load_audio_file(audio_path, target_sr=16000)
    
    # Convert to tensor for model input
    input_values = processor(audio, return_tensors="pt", sampling_rate=16000).input_values

    # Run inference
    with torch.no_grad():
        logits = model(input_values).logits

    predicted_ids = torch.argmax(logits, dim=-1)
    
    # Decode to text (English words)
    transcription = processor.batch_decode(predicted_ids)[0]
    
    # Chuy·ªÉn ƒë·ªïi sang IPA n·∫øu ƒë∆∞·ª£c y√™u c·∫ßu
    if output_ipa:
        ipa_transcription = convert_to_ipa(transcription)
        return {
            "raw": transcription,
            "ipa": ipa_transcription
        }
    
    return transcription